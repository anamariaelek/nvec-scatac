{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse gkm-SVM explain output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! pip install numpy==1.9 # failed\n",
    "#! pip install numpy==1.24.3\n",
    "#! pip install biopython=1.81\n",
    "#! pip install deeplift=0.6.13.0\n",
    "#! pip install modisco==0.5.16.0\n",
    "# # Successfully installed contourpy-1.1.1 cycler-0.12.1 fonttools-4.43.1 h5py-3.10.0 igraph-0.10.8 importlib-resources-6.1.0 joblib-1.3.2 kiwisolver-1.4.5 leidenalg-0.10.1 matplotlib-3.8.1 modisco-0.5.16.0 packaging-23.2 pillow-10.1.0 psutil-5.9.6 pyparsing-3.1.1 python-dateutil-2.8.2 scikit-learn-1.3.2 scipy-1.11.3 six-1.16.0 texttable-1.7.0 threadpoolctl-3.2.0 tqdm-4.66.1 zipp-3.17.0\n",
    "#! pip install tensorflow==2.13.0\n",
    "#! pip install ipykernel==6.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "conda activate tfmodisco"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_encode_along_channel_axis(sequence):\n",
    "    to_return = np.zeros((len(sequence),4), dtype=np.int8)\n",
    "    seq_to_one_hot_fill_in_array(zeros_array=to_return,\n",
    "                                 sequence=sequence, one_hot_axis=1)\n",
    "    return to_return\n",
    "\n",
    "def seq_to_one_hot_fill_in_array(zeros_array, sequence, one_hot_axis):\n",
    "    assert one_hot_axis==0 or one_hot_axis==1\n",
    "    if (one_hot_axis==0):\n",
    "        assert zeros_array.shape[1] == len(sequence)\n",
    "    elif (one_hot_axis==1): \n",
    "        assert zeros_array.shape[0] == len(sequence)\n",
    "    #will mutate zeros_array\n",
    "    for (i,char) in enumerate(sequence):\n",
    "        if (char==\"A\" or char==\"a\"):\n",
    "            char_idx = 0\n",
    "        elif (char==\"C\" or char==\"c\"):\n",
    "            char_idx = 1\n",
    "        elif (char==\"G\" or char==\"g\"):\n",
    "            char_idx = 2\n",
    "        elif (char==\"T\" or char==\"t\"):\n",
    "            char_idx = 3\n",
    "        elif (char==\"N\" or char==\"n\"):\n",
    "            continue #leave that pos as all 0's\n",
    "        else:\n",
    "            raise RuntimeError(\"Unsupported character: \"+str(char))\n",
    "        if (one_hot_axis==0):\n",
    "            zeros_array[char_idx,i] = 1\n",
    "        elif (one_hot_axis==1):\n",
    "            zeros_array[i,char_idx] = 1\n",
    "\n",
    "def normalize_scores(impscores, hyp_impscores, onehot_data):\n",
    "  #normalize the hyp scores such that, at each position, hypothetical importance\n",
    "  # scores that have the same sign as the original importance score all sum\n",
    "  # up to the original importance score value. The rationale is that if\n",
    "  # multiple different bases at a position could produce a similar score,\n",
    "  # the specific identity of each individual base is less important.\n",
    "  #Empirically, hypothetical scores like these appear to work better for\n",
    "  # motif discovery. Using normalized importance scores derived by taking\n",
    "  # the elementwise product of the normalized hypothetical scores and\n",
    "  # the one-hot encoding also seems to reduce noise.\n",
    "  normed_hyp_impscores = []\n",
    "  normed_impscores = []\n",
    "  for i in range(len(impscores)):\n",
    "      imp_score_each_pos = np.sum(impscores[i],axis=-1)\n",
    "      imp_score_sign_each_pos = np.sign(imp_score_each_pos)\n",
    "      hyp_scores_same_sign_mask = (np.sign(hyp_impscores[i])\n",
    "                                   *imp_score_sign_each_pos[:,None] > 0)\n",
    "      hyp_scores_same_sign_imp_scores_sum = np.sum(\n",
    "          hyp_impscores[i]*hyp_scores_same_sign_mask,axis=-1)\n",
    "      norm_ratio = imp_score_each_pos/hyp_scores_same_sign_imp_scores_sum\n",
    "      norm_hyp = hyp_impscores[i]*norm_ratio[:,None]\n",
    "      normed_hyp_impscores.append(norm_hyp)\n",
    "      normed_impscores.append(norm_hyp*onehot_data[i])\n",
    "  return normed_impscores, normed_hyp_impscores\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import FASTA sequences used with `gkmexplain`, importance scores and hypothetical importance scores, and save input arrays for TFMoDisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir = \"/home/anamaria/cluster/aelek/proj/scATAC_nvec_v2/clustering/SEACells/Results/ArchRProj_Nvec_TSS4_frag200/ArchRProj/\"\n",
    "dir_gkm = dir+\"gkmSVM/PeakDifferential_cell_type/03/\"\n",
    "dir_fig = dir+\"Plots/08_gkmSVM/PeakDifferential_cell_type/03/gkmekplain\"\n",
    "dir_mod = dir+\"tfmodisco/\"\n",
    "\n",
    "\n",
    "explain_files = [filename for filename in os.listdir(dir_gkm) if filename.endswith('.explain.fasta')]\n",
    "models = [x.replace('.explain.fasta', '') for x in explain_files]\n",
    "models = sorted(models)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Bio import SeqIO\n",
    "\n",
    "for model in models:\n",
    "    print(f\"Model: {model}\")\n",
    "    # Read fasta sequences\n",
    "    fasta_seqs=[]\n",
    "    # with open(dir_gkm+model+\".explain.fasta\") as handle:\n",
    "    with open(dir_gkm+\"all.fasta\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            fasta_seqs.append(str(record.seq))\n",
    "    # Filter out any sequences that contain 'N's\n",
    "    onehot_data = [np.array(one_hot_encode_along_channel_axis(x)) for x in fasta_seqs] #  if ('N' not in x)\n",
    "    print(f\"\\tNum onehot sequences: {len(onehot_data)}\")\n",
    "    # Read in the importance scores and hypothetical importance scores\n",
    "    hyp_impscores = [w[0] for w in zip([\n",
    "        np.array( [[float(z) for z in y.split(\",\")]\n",
    "                    for y in x.rstrip().split(\"\\t\")[2].split(\";\")])\n",
    "        for x in open(dir_gkm+model+\".explain.hyp.txt\")\n",
    "    ],fasta_seqs)]# if 'N' not in w[1]]\n",
    "    # Read in the hypothetical importance scores\n",
    "    impscores = [w[0] for w in zip([\n",
    "        np.array( [[float(z) for z in y.split(\",\")]\n",
    "                    for y in x.rstrip().split(\"\\t\")[2].split(\";\")])\n",
    "        for x in open(dir_gkm+model+\".explain.txt\")\n",
    "    ],fasta_seqs)]# if 'N' not in w[1]]\n",
    "    # Perform a sanity check to make sure that the importance score are the same as the\n",
    "    # hypothetical scores multiplied elementwise with the one-hot encoding\n",
    "    #assert (np.max([np.max(np.abs(z*y - x))\n",
    "    #                for x,y,z in zip(impscores,\n",
    "    #                                onehot_data,\n",
    "    #                                hyp_impscores)]))==0\n",
    "    # Save importance scores\n",
    "    impscores = [x[0:250] for x in impscores]\n",
    "    impscores_arr = np.array(impscores)\n",
    "    impscores_arr = np.swapaxes(impscores_arr, 1, 2)\n",
    "    np.savez(os.path.join(dir_mod, model+\"_impscore.npz\"), impscores_arr)\n",
    "    # Save hypothetical importance scores\n",
    "    hyp_impscores = [x[0:250] for x in hyp_impscores]\n",
    "    hyp_impscores_arr = np.array(hyp_impscores)\n",
    "    hyp_impscores_arr = np.swapaxes(hyp_impscores_arr, 1, 2)\n",
    "    np.savez(os.path.join(dir_mod, model+\"_hyp_impscores.npz\"), hyp_impscores_arr)\n",
    "    # Save one-hot data\n",
    "    onehot_data = [x[0:250] for x in onehot_data]\n",
    "    onehot_data_arr = np.array(onehot_data)\n",
    "    onehot_data_arr = np.swapaxes(onehot_data_arr, 1, 2)\n",
    "    np.savez(os.path.join(dir_mod, model+\"_onehot_data.npz\"), onehot_data_arr)\n",
    "np.savez(os.path.join(dir_mod, \"onehot_data.npz\"), onehot_data_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_impscores, normed_hyp_impscores = normalize_scores(\n",
    "impscores=impscores, hyp_impscores=hyp_impscores, onehot_data=onehot_data)\n",
    "\n",
    "# Normalize importance scores\n",
    "imp_dir=os.path.join(dir, model+\"_norm_impscores\")\n",
    "os.makedirs(imp_dir, exist_ok=True)\n",
    "\n",
    "for i, peak in enumerate(normed_impscores):\n",
    "    peak_arr = np.array(peak)\n",
    "    fn = os.path.join(imp_dir,  f\"norm_impscores_{i}.txt\")\n",
    "    np.savetxt(fn, peak_arr, delimiter=\"\\t\")\n",
    "\n",
    "for i, peak in enumerate(normed_hyp_impscores):\n",
    "    peak_arr = np.array(peak)\n",
    "    fn = os.path.join(imp_dir,  f\"norm_hyp_impscores_{i}.txt\")\n",
    "    np.savetxt(fn, peak_arr, delimiter=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a few sequences as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modisco.visualization import viz_sequence\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('default')\n",
    "\n",
    "def plot_weights_pdf(fn, array,\n",
    "                     figsize=(20,2),\n",
    "                     **kwargs):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111) \n",
    "    viz_sequence.plot_weights_given_ax(ax=ax, array=array,**kwargs)\n",
    "    plt.savefig(fn)\n",
    "\n",
    "sorted_indices = [x[0] for x in\n",
    "                  sorted(enumerate([np.sum(x) for x in impscores]),\n",
    "                         key=lambda x: -x[1])]\n",
    "\n",
    "for idx in sorted_indices[0:10]:\n",
    "    plot_weights_pdf(dir_fig+model+\"_weights_\"+str(idx)+\".pdf\", normed_impscores[idx], subticks_frequency=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run tfmodisco-lite from CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = np.load(os.path.join(dir_mod, model+\"_impscore.npz\"))\n",
    "ohe = np.load(os.path.join(dir_mod, model+\"_onehot_data.npz\"))\n",
    "\n",
    "for key in attr.files:\n",
    "    print(f\"The shape of the attribution scores is {attr[key].shape}\")\n",
    "\n",
    "for key in ohe.files:\n",
    "    print(f\"The shape of the one-hot encoding is {ohe[key].shape}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "work_dir=\"/home/anamaria/cluster/aelek/proj/scATAC_nvec_v2/clustering/SEACells/\"\n",
    "dir_gkm=${work_dir}\"Results/ArchRProj_Nvec_TSS4_frag200/ArchRProj/gkmSVM/PeakDifferential_cell_type/03/\"\n",
    "dir_mod=${work_dir}\"Results/ArchRProj_Nvec_TSS4_frag200/ArchRProj/tfmodisco/\"\n",
    "\n",
    "#model=\"adult_cnidocyte\"\n",
    "model=\"adult_neuron_GATA_Islet_1\" # problem with ARCH513\n",
    "atr=${model}\"_hyp_impscores.npz\"\n",
    "ohe=${model}\"_onehot_data.npz\"\n",
    "res=${model}\"_modisco_hyp.h5\"\n",
    "modisco motifs -s ${dir_mod}\"/\"${ohe} -a ${dir_mod}\"/\"${atr} -n 4000 -w 124 -o ${dir_mod}\"/\"${res}\n",
    "\n",
    "# archetypes\n",
    "mot_dir=${work_dir}\"/Results/ArchRProj_Nvec_TSS4_frag200/ArchRProj/Archetypes/\"\n",
    "mot_fn=\"motif-archetypes-PPM-PCCnorm-0.8-IC0.5-8bp-pwms-tfs.meme\"\n",
    "ln -s ${mot_dir}\"/\"${mot_fn} ${dir_mod}\"/\"${mot_fn}\n",
    "rep=${model}\"_modisco_hyp_report\"\n",
    "\n",
    "# jaspar motifs\n",
    "mot_fn=\"JASPAR2020_CORE_non-redundant_pfms_meme.txt\"\n",
    "rep=${model}\"_modisco_hyp_jaspar_report\"\n",
    "\n",
    "# homer motifs\n",
    "mot_fn=\"homer.motifs.meme\"\n",
    "rep=${model}\"_modisco_hyp_homer_report\"\n",
    "\n",
    "# report\n",
    "modisco report -i ${dir_mod}\"/\"${res} -o${dir_mod}\"/\"${rep} -s \"\" -m ${dir_mod}\"/\"${mot_fn} -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run modified tfmodisco-lite workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save importance scores of all models for all sequences into an Ordered Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "import modisco.util\n",
    "\n",
    "\n",
    "dir=\"/home/anamaria/cluster/aelek/proj/scATAC_nvec_v2/clustering/SEACells/Results/ArchRProj_Nvec_TSS4_frag200/ArchRProj/\"\n",
    "dir_gkm = dir+\"gkmSVM/PeakDifferential_cell_type/03/\"\n",
    "dir_fig = dir+\"Plots/08_gkmSVM/PeakDifferential_cell_type/03/tfmodisco/\"\n",
    "dir_mod = dir+\"tfmodisco/\"\n",
    "\n",
    "#explain_files = [filename for filename in os.listdir(dir_gkm) if filename.endswith('.explain.fasta')]\n",
    "#models = [x.replace('.explain.fasta', '') for x in explain_files]\n",
    "#models = sorted(models)\n",
    "\n",
    "npz_files = [filename for filename in os.listdir(dir_mod) if filename.endswith('_impscore.npz')]\n",
    "models = [x.replace('_impscore.npz', '') for x in npz_files]\n",
    "models = sorted(models)\n",
    "\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "task_to_hyp_scores = OrderedDict()\n",
    "\n",
    "for model_id, model in enumerate(models):\n",
    "    print(f\"Model: {model}\")\n",
    "    attr = np.load(os.path.join(dir_mod, model+\"_impscore.npz\"))\n",
    "    attr = attr[attr.files[0]] \n",
    "    print(f\"{attr.shape[0]} importance scores\")\n",
    "    task_to_hyp_scores[model] = []\n",
    "    for seq_id in range(len(attr)):\n",
    "        task_to_hyp_scores[model].append(np.transpose(attr[seq_id]))\n",
    "\n",
    "# save\n",
    "import pickle\n",
    "pickle.dump(\n",
    "    task_to_hyp_scores,\n",
    "    open(os.path.join(dir_mod, \"task_to_hyp_scores.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_regions = 3_000\n",
    "params = dict(\n",
    "    sliding_window_size=15,\n",
    "    flank_size=5,\n",
    "    min_metacluster_size=100,\n",
    "    weak_threshold_for_counting_sign=0.8,\n",
    "    target_seqlet_fdr=0.2,\n",
    "    min_passing_windows_frac=0.03,\n",
    "    max_passing_windows_frac=0.2,\n",
    "    n_leiden_runs=50,\n",
    "    n_leiden_iterations=-1,\n",
    "    min_overlap_while_sliding=0.7,\n",
    "    nearest_neighbors_to_compute=500,\n",
    "    affmat_correlation_threshold=0.15,\n",
    "    tsne_perplexity=10.0,\n",
    "    frac_support_to_trim_to=0.2,\n",
    "    min_num_to_trim_to=30,\n",
    "    trim_to_window_size=12,\n",
    "    initial_flank_to_add=5,\n",
    "    prob_and_pertrack_sim_merge_thresholds=[(0.8,0.8), (0.5, 0.85), (0.2, 0.9)],\n",
    "    prob_and_pertrack_sim_dealbreaker_thresholds=[(0.4, 0.75), (0.2,0.8), (0.1, 0.85), (0.0,0.9)],\n",
    "    subcluster_perplexity=50,\n",
    "    merging_max_seqlets_subsample=300,\n",
    "    final_min_cluster_size=10,\n",
    "    min_ic_in_window=0.6,\n",
    "    min_ic_windowsize=6,\n",
    "    ppm_pseudocount=0.001,\n",
    "    number_of_seqlets_to_sample = n_top_regions * 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom modisco-lite workflow by Seppe (run this from `~/lcb/aelek/gkmsvm_nvec/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "source /staging/leuven/stg_00002/lcb/sdewin/Programs/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate tfmodsico-lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get pattern for all cell types (tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "with open(os.path.join(\"task_to_hyp_scores.pkl\"), \"rb\") as f:\n",
    "    task_to_hyp_scores = pickle.load(f)\n",
    "\n",
    "tasks = list(task_to_hyp_scores.keys())\n",
    "\n",
    "import numpy as np\n",
    "seq_onehots_f = np.load(\"onehot_data.npz\")\n",
    "seq_onehots = seq_onehots_f['arr_0']\n",
    "seq_onehots = np.transpose(seq_onehots_f['arr_0'], (0, 2, 1))\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/staging/leuven/stg_00002/lcb/sdewin/PhD/python_modules/custom_modisco/src\")\n",
    "from custom_modisco import workflow\n",
    "\n",
    "# get patterns\n",
    "topics_to_patterns = workflow.get_patterns_for_all_tasks(\n",
    "    tasks = tasks,\n",
    "    one_hot = seq_onehots,\n",
    "    task_to_hyp_scores = task_to_hyp_scores,\n",
    "    n_cpu = 8\n",
    ")\n",
    "\n",
    "# save\n",
    "pickle.dump(\n",
    "    topics_to_patterns,\n",
    "    open(os.path.join(\"topics_to_patterns.pkl\"), \"wb\"))\n",
    "\n",
    "# ping me\n",
    "import os\n",
    "import requests\n",
    "TOKEN=\"7149163576:AAGPh9YitHcQavs4qu34ZVjvmyeKU189a-4\"\n",
    "chat_id=\"1418688827\"\n",
    "MESSAGE=\"TF Modisco is done!\"\n",
    "url=\"https://api.telegram.org/bot\" + TOKEN + \"/sendMessage?chat_id=\" + chat_id + \"&text=\" + MESSAGE\n",
    "requests.get(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a single list with all patterns and trim patterns by information content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_patterns, neg_patterns = [], []\n",
    "for topic_patterns in topics_to_patterns:\n",
    "        p, n = topic_patterns\n",
    "        if p is not None:\n",
    "            pos_patterns.extend(p)\n",
    "        if n is not None:\n",
    "            neg_patterns.extend(n)\n",
    "\n",
    "all_patterns = [*pos_patterns, *neg_patterns]\n",
    "\n",
    "trimmed_patterns = [\n",
    "    workflow.trim_pattern_by_ic(pattern, 0.1) for pattern in all_patterns\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get seqlets for all regions and all tasks/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlets = workflow.get_seqlets_for_all_tasks(\n",
    "    one_hot = seq_onehots,\n",
    "    task_to_hyp_scores = task_to_hyp_scores,\n",
    "    flank_size = 10\n",
    ")\n",
    "\n",
    "# ping me\n",
    "import os\n",
    "import requests\n",
    "TOKEN=\"7149163576:AAGPh9YitHcQavs4qu34ZVjvmyeKU189a-4\"\n",
    "chat_id=\"1418688827\"\n",
    "MESSAGE=f\"Getting seqlets is done!\"\n",
    "url=\"https://api.telegram.org/bot\" + TOKEN + \"/sendMessage?chat_id=\" + chat_id + \"&text=\" + MESSAGE\n",
    "requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Match seqlets to identified patterns (based on similarities of the pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_modisco import hit_scoring\n",
    "match_scores_patterns, fwd_rev_pattern = hit_scoring.match_score_seqlets_patterns(\n",
    "        seqlets, trimmed_patterns)\n",
    "\n",
    "# ping me\n",
    "import os\n",
    "import requests\n",
    "TOKEN=\"7149163576:AAGPh9YitHcQavs4qu34ZVjvmyeKU189a-4\"\n",
    "chat_id=\"1418688827\"\n",
    "MESSAGE=f\"Matching seqlets to patterns is done!\"\n",
    "url=\"https://api.telegram.org/bot\" + TOKEN + \"/sendMessage?chat_id=\" + chat_id + \"&text=\" + MESSAGE\n",
    "requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Threshold the similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "thresholds = np.zeros(len(trimmed_patterns))\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i, pattern in tqdm(enumerate(trimmed_patterns), total = len(trimmed_patterns)):\n",
    "        thresholds[i] = hit_scoring.get_sim_threshold_ROC(\n",
    "                match_scores = list(match_scores_patterns[i, :, 0]),\n",
    "                pattern = pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Calculate contribution scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrib_scores = hit_scoring.contrib_score_seqlets_pattern(\n",
    "    seqlets, trimmed_patterns, match_scores_patterns)\n",
    "\n",
    "# save\n",
    "pickle.dump(\n",
    "    contrib_scores,\n",
    "    open(os.path.join(\"contrib_scores.pkl\"), \"wb\"))\n",
    "\n",
    "# ping me\n",
    "import os\n",
    "import requests\n",
    "TOKEN=\"7149163576:AAGPh9YitHcQavs4qu34ZVjvmyeKU189a-4\"\n",
    "chat_id=\"1418688827\"\n",
    "MESSAGE=f\"Calculation of contribution scores is done!\"\n",
    "url=\"https://api.telegram.org/bot\" + TOKEN + \"/sendMessage?chat_id=\" + chat_id + \"&text=\" + MESSAGE\n",
    "requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Get hits on regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_region_hits = workflow.get_hits(\n",
    "        patterns = trimmed_patterns,\n",
    "        seqlets = seqlets,\n",
    "        fwd_rev_pattern = fwd_rev_pattern,\n",
    "        match_scores = match_scores_patterns,\n",
    "        contrib_scores = contrib_scores,\n",
    "        match_score_thresholds = thresholds,\n",
    "        contrib_threshold_quant = 0.60)\n",
    "\n",
    "# save\n",
    "df_region_hits.to_csv(\n",
    "        os.path.join(\"df_region_hits.tsv\"),\n",
    "        sep = '\\t', index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# load predictions\n",
    "#with open(os.path.join(\"prediction_score.pkl\"), \"rb\") as f:\n",
    "#    prediciton_scores = pickle.load(f)\n",
    "\n",
    "# load TFModisco patterns results\n",
    "with open(os.path.join(\"topics_to_patterns.pkl\"), \"rb\") as f:\n",
    "    topic_to_patterns = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(\"contrib_scores.pkl\"), \"rb\") as f:\n",
    "    contrib_scores =  pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modiscolite as modisco\n",
    "import numpy as np\n",
    "\n",
    "def flatten_list(l):\n",
    "        return [item for sublist in l for item in sublist]\n",
    "\n",
    "pos_patterns, neg_patterns = [], []\n",
    "for topic_patterns in topic_to_patterns:\n",
    "        p, n = topic_patterns\n",
    "        if p is not None:\n",
    "            pos_patterns.extend(p)\n",
    "        if n is not None:\n",
    "            neg_patterns.extend(n)\n",
    "\n",
    "affmat_pos = modisco.affinitymat.jaccard_from_seqlets(\n",
    "      seqlets = pos_patterns,\n",
    "      min_overlap = 0.9,\n",
    "      seqlet_neighbors = np.array([list(range(len(pos_patterns))) for x in pos_patterns]))\n",
    "affmat_neg = modisco.affinitymat.jaccard_from_seqlets(\n",
    "      seqlets = neg_patterns,\n",
    "      min_overlap = 0.9,\n",
    "      seqlet_neighbors = np.array([list(range(len(neg_patterns))) for x in neg_patterns]))\n",
    "\n",
    "from modiscolite.tfmodisco import _density_adaptation\n",
    "csr_density_adapted_affmat_pos = _density_adaptation(\n",
    "        affmat_pos, np.array([list(range(len(pos_patterns))) for x in pos_patterns]),\n",
    "        100)\n",
    "csr_density_adapted_affmat_neg = _density_adaptation(\n",
    "        affmat_neg, np.array([list(range(len(neg_patterns))) for x in neg_patterns]),\n",
    "        100)\n",
    "\n",
    "cluster_indices_pos = modisco.cluster.LeidenCluster(\n",
    "        csr_density_adapted_affmat_pos,\n",
    "        n_seeds=50,\n",
    "        n_leiden_iterations=-1)\n",
    "cluster_indices_neg = modisco.cluster.LeidenCluster(\n",
    "        csr_density_adapted_affmat_neg,\n",
    "        n_seeds=50,\n",
    "        n_leiden_iterations=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logomaker\n",
    "from tqdm import tqdm\n",
    "\n",
    "# sort positive patterns by cluster membership\n",
    "cluster_indices_pos_idx = np.argsort(cluster_indices_pos)\n",
    "cluster_indices_pos_sort = cluster_indices_pos[cluster_indices_pos_idx]\n",
    "\n",
    "# plot patterns for each cluster\n",
    "for cluster_idx in np.unique(cluster_indices_pos_sort):\n",
    "        print(cluster_idx)\n",
    "        # get members of the cluster\n",
    "        cluster_indices_pos_idx_cl = cluster_indices_pos_idx[cluster_indices_pos == cluster_idx]\n",
    "        # prepare figure\n",
    "        fig, axs = plt.subplots(\n",
    "                nrows = len(cluster_indices_pos_idx_cl),\n",
    "                figsize = (8, len(cluster_indices_pos_idx_cl) * 3))\n",
    "        # loop over all patterns in the cluster\n",
    "        for pattern_idx, ax in tqdm(zip(cluster_indices_pos_idx_cl, axs.ravel()), total = len(pos_patterns)):\n",
    "                pattern = pos_patterns[pattern_idx]\n",
    "                df = pd.DataFrame(pattern.contrib_scores, columns=['A', 'C', 'G', 'T'])\n",
    "                df.index.name = 'pos'\n",
    "                crp_logo = logomaker.Logo(df, ax=ax)\n",
    "                crp_logo.style_spines(visible=False)\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.set_title(f'pattern_{pattern_idx}; cluster {cluster_idx}')\n",
    "                ax.set_ylim(min(df.sum(axis=1).min(), 0), df.sum(axis=1).max())\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(os.path.join(f'plots/all_patterns_pos_cluster{cluster_idx}.pdf'))\n",
    "\n",
    "# sort negative patterns by cluster membership\n",
    "cluster_indices_neg_idx = np.argsort(cluster_indices_neg)\n",
    "cluster_indices_neg_sort = cluster_indices_neg[cluster_indices_neg_idx]\n",
    "\n",
    "# plot patterns for each cluster\n",
    "for cluster_idx in np.unique(cluster_indices_neg_sort):\n",
    "        print(cluster_idx)\n",
    "        # get members of the cluster\n",
    "        cluster_indices_neg_idx_cl = cluster_indices_neg_idx[cluster_indices_neg == cluster_idx]\n",
    "        # prepare figure\n",
    "        fig, axs = plt.subplots(\n",
    "                nrows = len(cluster_indices_neg_idx_cl),\n",
    "                figsize = (8, len(cluster_indices_neg_idx_cl) * 3))\n",
    "        # loop over all patterns in the cluster\n",
    "        for pattern_idx, ax in tqdm(zip(cluster_indices_neg_idx_cl, axs.ravel()), total = len(pos_patterns)):\n",
    "                pattern = neg_patterns[pattern_idx]\n",
    "                df = pd.DataFrame(pattern.contrib_scores, columns=['A', 'C', 'G', 'T'])\n",
    "                df.index.name = 'pos'\n",
    "                crp_logo = logomaker.Logo(df, ax=ax)\n",
    "                crp_logo.style_spines(visible=False)\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.set_title(f'pattern_{pattern_idx}; cluster {cluster_idx}')\n",
    "                ax.set_ylim(min(df.sum(axis=1).min(), 0), df.sum(axis=1).max())\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(os.path.join(f'plots/all_patterns_neg_cluster{cluster_idx}.pdf'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all positive patterns together, and all negative patterns together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import logomaker\n",
    "from tqdm import tqdm\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "        nrows = len(pos_patterns),\n",
    "        figsize = (8, len(pos_patterns) * 4))\n",
    "\n",
    "for pattern_idx, ax in tqdm(zip(np.argsort(cluster_indices_pos), axs.ravel()), total = len(pos_patterns)):\n",
    "        pattern = pos_patterns[pattern_idx]\n",
    "        df = pd.DataFrame(pattern.contrib_scores, columns=['A', 'C', 'G', 'T'])\n",
    "        df.index.name = 'pos'\n",
    "        crp_logo = logomaker.Logo(df, ax=ax)\n",
    "        crp_logo.style_spines(visible=False)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.set_title(f'pattern_{pattern_idx}')\n",
    "        ax.set_ylim(min(df.sum(axis=1).min(), 0), df.sum(axis=1).max())\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join('plots/all_patterns_pos.pdf'))\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "        nrows = len(neg_patterns),\n",
    "        figsize = (8, len(neg_patterns) * 4))\n",
    "\n",
    "for pattern_idx, ax in tqdm(zip(np.argsort(cluster_indices_neg), axs.ravel()), total = len(neg_patterns)):\n",
    "        pattern = neg_patterns[pattern_idx]\n",
    "        df = pd.DataFrame(pattern.contrib_scores, columns=['A', 'C', 'G', 'T'])\n",
    "        df.index.name = 'pos'\n",
    "        crp_logo = logomaker.Logo(df, ax=ax)\n",
    "        crp_logo.style_spines(visible=False)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.set_title(f'pattern_{pattern_idx}')\n",
    "        ax.set_ylim(min(df.sum(axis=1).min(), 0), df.sum(axis=1).max())\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join('plots/all_patterns_neg.pdf'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save trimmed patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from custom_modisco.workflow import trim_pattern_by_ic\n",
    "\n",
    "background = np.array([0.30, 0.18, 0.21, 0.31])\n",
    "\n",
    "# positive patterns\n",
    "patterns = np.argsort(cluster_indices_pos)\n",
    "pwm_list = []\n",
    "\n",
    "for pattern_idx in tqdm(patterns, total = len(patterns)):\n",
    "    untrimmed_pattern = pos_patterns[pattern_idx]\n",
    "    try:\n",
    "        trimmed_pattern = trim_pattern_by_ic(pattern = untrimmed_pattern, min_v = 0.25, background = background)\n",
    "    except:\n",
    "        trimmed_pattern = untrimmed_pattern\n",
    "    df = pd.DataFrame(trimmed_pattern.contrib_scores, columns = ['A', 'C', 'G', 'T']).fillna(0)\n",
    "    df[df < 0] = 0\n",
    "    pwm_list.append(df)\n",
    "\n",
    "# save txt file\n",
    "with open(os.path.join('patterns_pos.txt'),'w') as txt_file:\n",
    "    for i in range(len(pwm_list)):\n",
    "        pwm=pwm_list[i]\n",
    "        pwm.to_csv(txt_file, sep=\"\\t\", header=False, index=False)\n",
    "        txt_file.write(\"\\n\")\n",
    "\n",
    "# save meme file\n",
    "#with open(os.path.join(tfmodisco_dir, 'patterns_pos_.meme'),'w') as meme_file:\n",
    "#    meme_file.write(\"MEME version 5.0\\n\\nALPHABET= ACGT\\n\\nstrands: + -\\n\\n\")\n",
    "#    for i in range(len(pwm_list)):\n",
    "#        pwm=pwm_list[i]\n",
    "#        meme_file.write(f\"MOTIF pos_pattern_{patterns[i]}\\n\")\n",
    "#        meme_file.write(f\"letter-probability matrix: alength= 4 w= {len(pwm.columns)} nsites= 20\\n\")\n",
    "#        pwm.to_csv(meme_file, sep=\"\\t\", header=False, index=False)\n",
    "#        meme_file.write(\"\\n\")\n",
    "\n",
    "with open(os.path.join('patterns_pos.names'),'w') as name_file:\n",
    "    for i in range(len(pwm_list)):\n",
    "        name_file.write(f'pos_pattern_{patterns[i]}\\n')\n",
    "\n",
    "# negative patterns\n",
    "patterns = np.argsort(cluster_indices_neg)\n",
    "pwm_list = []\n",
    "\n",
    "for pattern_idx in tqdm(patterns, total = len(patterns)):\n",
    "    untrimmed_pattern = neg_patterns[pattern_idx]\n",
    "    try:\n",
    "        trimmed_pattern = trim_pattern_by_ic(pattern = untrimmed_pattern, min_v = 0.25, background = background)\n",
    "    except:\n",
    "        trimmed_pattern = untrimmed_pattern\n",
    "    df = pd.DataFrame(trimmed_pattern.contrib_scores, columns = ['A', 'C', 'G', 'T']).fillna(0).abs()\n",
    "    pwm_list.append(df)\n",
    "\n",
    "# save txt file\n",
    "with open(os.path.join('patterns_neg.txt'),'w') as txt_file:\n",
    "    for i in range(len(pwm_list)):\n",
    "        pwm=pwm_list[i]\n",
    "        pwm.to_csv(txt_file, sep=\"\\t\", header=False, index=False)\n",
    "        txt_file.write(\"\\n\")\n",
    "\n",
    "# save meme file\n",
    "#with open(os.path.join(tfmodisco_dir, 'patterns_neg_.meme'),'w') as meme_file:\n",
    "#    meme_file.write(\"MEME version 5.0\\n\\nALPHABET= ACGT\\n\\nstrands: + -\\n\\n\")\n",
    "#    for i in range(len(pwm_list)):\n",
    "#        pwm=pwm_list[i]\n",
    "#        meme_file.write(f\"MOTIF neg_pattern_{patterns[i]}\\n\")\n",
    "#        meme_file.write(f\"letter-probability matrix: alength= 4 w= {len(pwm.columns)} nsites= 20\\n\")\n",
    "#        pwm.to_csv(meme_file, sep=\"\\t\", header=False, index=False)\n",
    "#        meme_file.write(\"\\n\")\n",
    "\n",
    "# save names\n",
    "with open(os.path.join('patterns_neg.names'),'w') as name_file:\n",
    "    for i in range(len(pwm_list)):\n",
    "        name_file.write(f'neg_pattern_{patterns[i]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match patterns to motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "motifs=\"/staging/leuven/stg_00002/lcb/aelek/data_nvec/motif-archetypes-PPM-PCCnorm-0.8-IC0.5-8bp-unique-pwms.meme\"\n",
    "id=\"archetypes\"\n",
    "tfmodisco_dir=\"/staging/leuven/stg_00002/lcb/aelek/gkmsvm_nvec/\"\n",
    "\n",
    "patterns=${tfmodisco_dir}\"/patterns_pos.txt\"\n",
    "outdir=${tfmodisco_dir}\"$(basename ${patterns})\"\n",
    "outdir=${outdir%%.txt}\n",
    "scripts/tomtom.sh ${patterns} ${motifs} ${id} ${outdir}\n",
    "\n",
    "patterns=${tfmodisco_dir}\"/patterns_neg.txt\"\n",
    "outdir=${tfmodisco_dir}\"$(basename ${patterns})\"\n",
    "outdir=${outdir%%.txt}\n",
    "scripts/tomtom.sh ${patterns} ${motifs} ${id} ${outdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate contribution of all patterns for all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from custom_modisco.workflow import get_pattern_contribution_for_all_tasks\n",
    "\n",
    "import numpy as np\n",
    "task_to_hyp_scores = {task: np.array(task_to_hyp_scores[task]) for task in task_to_hyp_scores.keys()}\n",
    "\n",
    "# calculate contribution for all patterns\n",
    "selected_patterns_pos = np.argsort(cluster_indices_pos)\n",
    "selected_patterns_neg = np.argsort(cluster_indices_neg)\n",
    "\n",
    "pattern_to_task_contribution = {}\n",
    "\n",
    "from tqdm import tqdm\n",
    "for pattern_id in tqdm(np.argsort(cluster_indices_pos), total = len(cluster_indices_pos)):\n",
    "        pattern = pos_patterns[pattern_id]\n",
    "        name = f\"pos_pattern_{pattern_id}\"\n",
    "        pattern_to_task_contribution[name] = get_pattern_contribution_for_all_tasks(\n",
    "                pattern = pattern,\n",
    "                one_hot = seq_onehots, task_to_hyp_scores = task_to_hyp_scores)\n",
    "\n",
    "for pattern_id in tqdm(np.argsort(cluster_indices_neg), total = len(cluster_indices_neg)):\n",
    "        pattern = neg_patterns[pattern_id]\n",
    "        name = f\"neg_pattern_{pattern_id}\"\n",
    "        pattern_to_task_contribution[name] = get_pattern_contribution_for_all_tasks(\n",
    "                pattern = pattern,\n",
    "                one_hot = seq_onehots, task_to_hyp_scores = task_to_hyp_scores)\n",
    "\n",
    "# save\n",
    "pickle.dump(\n",
    "    pattern_to_task_contribution,\n",
    "    open(os.path.join(\"pattern_to_task_contribution.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make matrix with contributions for code tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from custom_modisco.visualization import plot_pattern\n",
    "\n",
    "# contributions\n",
    "with open(os.path.join(\"pattern_to_task_contribution.pkl\"), \"rb\") as f:\n",
    "    pattern_to_task_contribution = pickle.load(f)\n",
    "\n",
    "df_pattern_to_task_contribution = pd.DataFrame(pattern_to_task_contribution).T\n",
    "\n",
    "# matrix dimensions\n",
    "nrows = len(df_pattern_to_task_contribution.index)\n",
    "ncols = len(df_pattern_to_task_contribution.columns)\n",
    "\n",
    "# get contributions into matrix format\n",
    "df_pattern_to_task_contribution_max = pd.DataFrame(\n",
    "        index = df_pattern_to_task_contribution.index,\n",
    "        columns = df_pattern_to_task_contribution.columns).fillna(0)\n",
    "\n",
    "for pattern in df_pattern_to_task_contribution_max.index:\n",
    "        for task in df_pattern_to_task_contribution_max.columns:\n",
    "                df_pattern_to_task_contribution_max.loc[pattern, task] = abs(\n",
    "                        df_pattern_to_task_contribution.loc[pattern, task].contrib_scores\n",
    "                ).max() * np.sign(df_pattern_to_task_contribution.loc[pattern, task].contrib_scores.mean())\n",
    "\n",
    "# for each pattern, which topic it has max contribution to? to order patterns\n",
    "sorted_patterns = pd.DataFrame(df_pattern_to_task_contribution_max.idxmax(axis = 1)).reset_index().set_index(0)\n",
    "sorted_patterns = sorted_patterns['index'].to_list()\n",
    "\n",
    "# order, map topics to cell types and save table\n",
    "df_pattern_to_task_contribution_max = df_pattern_to_task_contribution_max.loc[sorted_patterns].copy()\n",
    "df_pattern_to_task_contribution_max.to_csv(os.path.join('pattern_to_task_contribution.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code tables for selected patterns based on expression correlation (see `.Rmd`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-MoDISco"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate dinuc shuffled sequences for computing null distribution of importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51879"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deeplift.dinuc_shuffle import dinuc_shuffle\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "num_dinuc_shuffled_seqs = 1000\n",
    "\n",
    "# Generate the dinucleotide shuffled sequences\n",
    "fasta_seqs_test=[]\n",
    "with open(dir+model+\".explain.fasta\") as handle:\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        fasta_seqs_test.append(str(record.seq))\n",
    "        \n",
    "fasta_seqs_no_N = [x.rstrip() for x in fasta_seqs_test if ('N' not in x)]\n",
    "\n",
    "# Save\n",
    "open(dir+model+\".dnshuff.fasta\", 'w').write(\n",
    " \"\\n\".join([\">seq\"+str(i)+\"\\n\"+dinuc_shuffle(np.random.choice(fasta_seqs_no_N).rstrip())\n",
    "            for i in range(num_dinuc_shuffled_seqs)]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load importance scores for shuffled sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnshuff_impscores = [np.array( [[float(z) for z in y.split(\",\")]\n",
    "                           for y in x.rstrip().split(\"\\t\")[2].split(\";\")])\n",
    "                     for x in open(dir+model+\".explain.dnshuff.txt\")]\n",
    "dnshuff_perposimp = [np.sum(x,axis=-1) for x in dnshuff_impscores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modisco.visualization import viz_sequence\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('default')\n",
    "import h5py\n",
    "import numpy as np\n",
    "import modisco\n",
    "\n",
    "#I sum up the null distribution importance scores at each position\n",
    "# to get the perpos importance scores\n",
    "# of the null distribution. I decided to use\n",
    "# the unnormalized dnshuff_impscores (rather than normalized versions\n",
    "# of these scores) as this made for a more stringent null distribution.\n",
    "# Figuring out the best way to generate a null distribution for these scores\n",
    "# is still an open problem.\n",
    "dnshuff_perposimp = [np.sum(x,axis=-1) for x in dnshuff_impscores]\n",
    "\n",
    "tfmodisco_results = modisco.tfmodisco_workflow.workflow.TfModiscoWorkflow(\n",
    "    #target_seqlet_fdr=0.15,\n",
    "    min_passing_windows_frac=0.03,\n",
    "    max_passing_windows_frac=0.2,\n",
    "    seqlets_to_patterns_factory = modisco.tfmodisco_workflow.seqlets_to_patterns.TfModiscoSeqletsToPatternsFactory(\n",
    "        kmer_len=6, num_gaps=1, num_mismatches=0,\n",
    "   ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfmodisco_results = tfmodisco_results(\n",
    "    task_names=[model],\n",
    "    contrib_scores={model: impscores},\n",
    "    hypothetical_contribs={model: hyp_impscores},\n",
    "    one_hot=onehot_data,\n",
    "    null_per_pos_scores={model: dnshuff_perposimp}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import modisco.util\n",
    "grp = h5py.File(dir+model+\".results.hdf5\", \"w\")\n",
    "tfmodisco_results.save_hdf5(grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hdf5_results = h5py.File(dir+model+\".results.hdf5\",\"r\")\n",
    "\n",
    "metacluster_names = [\n",
    "    x.decode(\"utf-8\") for x in \n",
    "    list(hdf5_results[\"metaclustering_results\"]\n",
    "         [\"all_metacluster_names\"][:])]\n",
    "print(metacluster_names)\n",
    "#metacluster_grp = (hdf5_results[\"metacluster_idx_to_submetacluster_results\"][metacluster_name])\n",
    "#print(\"activity pattern:\",metacluster_grp[\"activity_pattern\"][:])\n",
    "#pattern = metacluster_grp[\"seqlets_to_patterns_result\"][\"patterns\"][pattern_name]\n",
    "#hdf5_results[][model+\"_hypothetical_contribs\"][\"fwd\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modisco.aggregator import TrimToBestWindowByIC\n",
    "\n",
    "trimmer = TrimToBestWindowByIC(\n",
    "    window_size=30,\n",
    "    onehot_track_name=[model+\"_contrib_scores\"],\n",
    "    bg_freq=np.array([0.25,0.25,0.25,0.25])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(tfmodisco_results.multitask_seqlet_creation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"num seqlets\",len(pattern.seqlets))\n",
    "print(\"fwd seq PWM\")\n",
    "viz_sequence.plot_weights(viz_sequence.ic_scale(\n",
    "    pattern[\"sequence\"].fwd, background=np.array([0.25,0.25,0.25,0.25])))\n",
    "print(\"Contrib scores\")\n",
    "viz_sequence.plot_weights(pattern[\"task0_contrib_scores\"].fwd)\n",
    "print(\"Hyp contrib scores\")\n",
    "viz_sequence.plot_weights(pattern[\"task0_hypothetical_contribs\"].fwd)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,pattern in enumerate(trimmer(tfmodisco_results.metaclustering_results.metaclusterer\n",
    "                                   .metacluster_idx_to_activity_pattern[0]\n",
    "                                   .seqlets_to_patterns_result.patterns)):\n",
    "    print(\"num seqlets\",len(pattern.seqlets))\n",
    "    print(\"fwd seq PWM\")\n",
    "    viz_sequence.plot_weights(viz_sequence.ic_scale(\n",
    "        pattern[\"sequence\"].fwd, background=np.array([0.25,0.25,0.25,0.25])))\n",
    "    print(\"Contrib scores\")\n",
    "    viz_sequence.plot_weights(pattern[\"task0_contrib_scores\"].fwd)\n",
    "    print(\"Hyp contrib scores\")\n",
    "    viz_sequence.plot_weights(pattern[\"task0_hypothetical_contribs\"].fwd)\n",
    "  \n",
    "    print(\"rev seq PWM\")\n",
    "    viz_sequence.plot_weights(viz_sequence.ic_scale(\n",
    "      pattern[\"sequence\"].rev, background=np.array([0.25,0.25,0.25,0.25])))\n",
    "    print(\"Contrib scores\")\n",
    "    viz_sequence.plot_weights(pattern[\"task0_contrib_scores\"].rev)\n",
    "    print(\"Hyp contrib scores\")\n",
    "    viz_sequence.plot_weights(pattern[\"task0_hypothetical_contribs\"].rev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
